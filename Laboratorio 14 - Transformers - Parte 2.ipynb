{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHopPtVaNF1K"
      },
      "source": [
        "# **Diplomado IA: Inteligencia Artificial II - Parte 1**. <br> Laboratorio 1: Redes Relacionales y Transformers\n",
        "---\n",
        "---\n",
        "\n",
        "**Profesor:**\n",
        "- Felipe del Río\n",
        "\n",
        "**Ayudante:**\n",
        "- Bianca del Solar\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIdAKAdELPSl"
      },
      "source": [
        "# **Instrucciones Generales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmhnKlt8Ns7A"
      },
      "source": [
        "El siguiente práctico será **individual**. Solo uno debe realizar la entrega. El formato de entregar es el **archivo .ipynb con todas las celdas ejecutadas**. Todas las preguntas deben ser respondida en celdas de texto. No se aceptará el _output_ de una celda de código como respuesta.\n",
        "\n",
        "**Nombre:** COMPLETAR\n",
        "\n",
        "**Fecha de entrega: Viernes 30 de Junio.**\n",
        "\n",
        "El siguiente práctico cuenta con 2 secciones donde cada una contendrá 1 o más actividades a realizar. Algunas actividades correspondrán a escribir código y otras a responder preguntas.\n",
        "\n",
        "**Importante.** Para facilitar su ejecución, cada sección puede ser ejecutada independientemente.\n",
        "\n",
        "Se recomienda **fuertemente** revisar las secciones donde se entrega código porque algunas actividades de código pueden reutilizar el mismo código pero con cambios en algunas líneas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEloa5uXLIPK"
      },
      "source": [
        "# **Agenda**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "DGlX5q_pQYRa"
      },
      "source": [
        ">[Diplomado IA: Inteligencia Artificial II - Parte 1.  Laboratorio 1: Redes Relacionales y Transformers](#scrollTo=tHopPtVaNF1K)\n",
        "\n",
        ">[Instrucciones Generales](#scrollTo=uIdAKAdELPSl)\n",
        "\n",
        ">[Agenda](#scrollTo=kEloa5uXLIPK)\n",
        "\n",
        ">[Parte III: Inspeccionando a CLIP](#scrollTo=ZS3cYFT2TWB9)\n",
        "\n",
        ">>[Preámbulo](#scrollTo=0dBeU4b818s4)\n",
        "\n",
        ">>[Cargamos el Modelo](#scrollTo=1D5aBia_ucDJ)\n",
        "\n",
        ">>[Classificación zero-shot utilizando CLIP](#scrollTo=IPrwC1iF8K0j)\n",
        "\n",
        ">>>[Dataset Food101](#scrollTo=IPrwC1iF8K0j)\n",
        "\n",
        ">>>[Actividad 3](#scrollTo=3tABpd-l6-xH)\n",
        "\n",
        ">>>[Dataset Stanford Cars](#scrollTo=qwbkd4RaAqYt)\n",
        "\n",
        ">>>[Actividad](#scrollTo=CO9kxJAf6SDO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS3cYFT2TWB9"
      },
      "source": [
        "# Parte III: Inspeccionando a CLIP\n",
        "\n",
        "Luego de haber visualizado el funcionamento interno de un transformer, utilizaremos CLIP para ver como se puede aprovechar al máximo esta arquitectura en problemas multi-modales, en este caso, que mezclan texto con imágenes.\n",
        "\n",
        "Como vimos en clases CLIP es un modelo que nos permite codificar tanto imágenes como texto en vectores de representación comparables entre ellos. Este nos permite ver cual es el texto que más se relaciona con una imágen determinada, y podemos aprovechar este mecanismo para clasificar imágenes con una mayor flexibilidad que en un modelo de clasificación tradicional. En esta tercera parte del laboratorio exploraremos como utilizar CLIP con este fin.\n",
        "\n",
        "<small>Este notebook fue basado en [el provisto por OpenAI](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb#scrollTo=uLFS29hnhlY4) con el fin de interacturar con CLIP</small>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dBeU4b818s4"
      },
      "source": [
        "## Preámbulo\n",
        "\n",
        "Primero debemos descargar, instalar e importar las distintas librerías que utilizaremos para este laboratorio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFo1IBx-x-rC"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCKW2hAUyK_4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage\n",
        "import random\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bold(text):\n",
        "    return '\\033[1m' + text + '\\033[0m'"
      ],
      "metadata": {
        "id": "ewqMPryrGgsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargamos el Modelo\n",
        "\n",
        "Luego cargaremos el modelo a utilizar. Utilizando la función `clip.available_models()` podemos listar los diferentes modelos displonibles. Pueden ver más detalles de lo que significa cada modelo en el [paper](https://arxiv.org/pdf/2103.00020.pdf) de CLIP.\n",
        "\n",
        "Pero, a grandes razgos, **RN** corresponde a un backbone de ResNet para el encoder visual, RN50 correspondría a una ResNet50, y cuando tiene el sufijo `xN`, `x4` por ejemplo, significa que el modelo está escalado para utilizar `N` veces más computo.\n",
        "\n",
        "Mientras que **ViT** corresponde a un backbone de Vision Transformer. El símbolo `-B` o `-L` corresponde al tamaño del modelo, Base y Large respectivamente y el sufijo `/32` corresponde a que los patches son de `32x32`.\n",
        "\n",
        "Para este laboratorio usaremos la versión `ViT-B/32`, que está basado en su totalidad en Transformers.\n",
        "\n"
      ],
      "metadata": {
        "id": "1D5aBia_ucDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "lprfPYqmuRmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero carguemos un modelo, no tan grande, para utilizar en esta actividad.\n",
        "\n",
        "El código a continuación descargará los pesos del modelo preentrenado de forma automática. Una vez cargados, podemos veamos algunas características relevantes del modelo."
      ],
      "metadata": {
        "id": "4lAKI4cAxadG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Wo7thJQ1so36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.to(device).eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Número de parámetros:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Resolución de Entrada:\", input_resolution)\n",
        "print(\"Tamaño del contexto:\", context_length)\n",
        "print(f\"Tamaño del vocabulario: {vocab_size:,}\")"
      ],
      "metadata": {
        "id": "wstNo8cnuRqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos acceder al preprocesamiento que CLIP realiza sobre las imágenes, en la variable `preprocess`. Este se basa en las transformaciones de PyTorch para transformar una imágen a un tensor normalizado que el modelo puede recibir (recuerden la clase anterior de data augmentation)."
      ],
      "metadata": {
        "id": "_IlihJQ70emr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "g6KiypHCuRtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classificación zero-shot utilizando CLIP\n",
        "\n",
        "Ya teniendo el modelo cargado, veamos como se comporta CLIP para clasificar dos set de datos con características distintas entre ellos.\n"
      ],
      "metadata": {
        "id": "IPrwC1iF8K0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Dataset Food101\n",
        "\n",
        "Primero probaremos en el dataset Food101. Este dataset consiste en 101 comidas diferentes y un total de 101.000 imágenes de estas."
      ],
      "metadata": {
        "id": "SI2I5ws81vvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import Food101\n",
        "\n",
        "food_dataset = Food101(os.path.expanduser(\"~/.cache\"), download=True)"
      ],
      "metadata": {
        "id": "emxDybfvuRv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Queries* para Predicción\n",
        "\n",
        "Primero que todo, debemos construir las *queries* que utilizaremos para realizar la clasificación. Recordemos que el modelo que vamos a utilizar está entrenado para entregar una correspondencia entre un texto y una imágen. Por esto, para efectuar clasificaciones tendremos que entregarle la clase que queremos clasificar en forma de texto y para esto usaremos unas *queries* predefinidas.\n",
        "\n",
        "A continuación, definiremos un *template* para construir nuestras *queries* y veremos algunos ejemplos de las que utilizaremos para clasificar el dataset de Food101."
      ],
      "metadata": {
        "id": "81efcz_FvBzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_template = 'A photo of {}, a type of food.'\n",
        "queries = [query_template.format(label) for label in food_dataset.classes]\n",
        "tokenized_queries = clip.tokenize(queries).to(device)"
      ],
      "metadata": {
        "id": "1X44_SYb8Yne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries[:10]:\n",
        "    print(query)"
      ],
      "metadata": {
        "id": "tiXbGXt1Dryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización de Predicciones\n",
        "\n",
        "Seleccione una imágen del dataset utilizando el parámetro `index` para ver las predicciones del modelo. Si prefiere elegir una imágen al azar, active el campo de `random_sample`."
      ],
      "metadata": {
        "id": "wHmZFDW1D2bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = True #@param {type:\"boolean\"}\n",
        "index = 0 #@param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "EyrKtLhYEFPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if random_sample:\n",
        "    index = random.randint(0, len(food_dataset))\n",
        "original_image, true_label = food_dataset[index]\n",
        "image = preprocess(original_image)\n",
        "image_input = image.unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "TOfxf23kBUdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = model.encode_text(tokenized_queries).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ],
      "metadata": {
        "id": "gp1n0v9l8Yrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos explorar algunos de los resultados que genera CLIP, a continuación veremos las dimensiones de los vectores de características producidos, tanto de texto como de la imagen, además de la matriz de similaridades entre los distintos textos e imágenes que le entregamos."
      ],
      "metadata": {
        "id": "y_n2ksg1Hjbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Dim features de la imágen   :', image_features.shape)\n",
        "print('Dim features del texto      :', text_features.shape)\n",
        "print('Dim matriz de similaridades :', text_probs.shape)"
      ],
      "metadata": {
        "id": "FBy_sGTpHieX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos visualmente como es la predicción de este modelo para el ejemplo seleccionado."
      ],
      "metadata": {
        "id": "OmoFEjZyADoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_label_name = food_dataset.classes[true_label]\n",
        "\n",
        "print(bold('Predicciones Top-5\\n'))\n",
        "print(bold(f'Clase          '), bold(f'Probabilidad'))\n",
        "for prob, label in zip(top_probs[0].tolist(), top_labels[0].tolist()):\n",
        "    label_name = food_dataset.classes[label]\n",
        "    if true_label_name == label_name:\n",
        "        print(bold(f'{label_name:15.15s} {prob:.4f}'))\n",
        "    else:\n",
        "        print(f'{label_name:15.15s} {prob:.4f}')\n",
        "\n",
        "print(bold(f'\\nClase verdadera: {true_label_name}'), end='\\n\\n')\n",
        "original_image"
      ],
      "metadata": {
        "id": "WSRmF0ykCx5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rendimiento del Modelo\n",
        "\n",
        "Ahora midamos el rendimiento que obtiene este modelo si lo usamos para clasificar en el set de test.\n",
        "\n",
        "Es importante recordar que el modelo que estamos usando no fue entrenado para clasificar en este, por lo que estamos usando una técnica zero-shot."
      ],
      "metadata": {
        "id": "Epi3aIt-AG7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def evaluate_model(model, dataset, queries, batch_size=512):\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    in_top1 = in_top5 = total = 0.\n",
        "    total_batches = len(test_dataset) // batch_size\n",
        "    for image_inputs, true_labels in tqdm(test_loader, total=total_batches):\n",
        "        image_inputs = image_inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image_inputs).float()\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            text_features = model.encode_text(queries).float()\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "        top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
        "\n",
        "        label_match = (top_labels == true_labels.unsqueeze(-1))\n",
        "        in_top1 += float(label_match[:,0].sum())\n",
        "        in_top5 += float(label_match.any(-1).sum())\n",
        "        total += true_labels.numel()\n",
        "\n",
        "    top1_acc = in_top1 / total\n",
        "    top5_acc = in_top5 / total\n",
        "\n",
        "    return top1_acc, top5_acc"
      ],
      "metadata": {
        "id": "NQo4RCDF0BAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "test_dataset = Food101(os.path.expanduser(\"~/.cache\"),\n",
        "                       split='test',\n",
        "                       transform=preprocess,\n",
        "                       download=True)"
      ],
      "metadata": {
        "id": "mCsAPmOEAfcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1_acc, top5_acc = evaluate_model(\n",
        "    model, test_dataset, tokenized_queries, batch_size=512)"
      ],
      "metadata": {
        "id": "rNdFkdYHz3hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1_acc = top1_acc * 100\n",
        "top5_acc = top5_acc * 100\n",
        "print(f'Top-1 Accuracy: {top1_acc:.2f}%')\n",
        "print(f'Top-5 Accuracy: {top5_acc:.2f}%')"
      ],
      "metadata": {
        "id": "08w0fGLRKo_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actividad 3\n",
        "\n",
        "Responda las siguientes preguntas"
      ],
      "metadata": {
        "id": "3tABpd-l6-xH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3l1DFguvM8M"
      },
      "source": [
        "1. **En la celda en donde obtuvimos las dimensiones de distintos resultados generados por el modelo.**\n",
        "\n",
        "A qué corresponde el valor de la última dimension de los features de texto e imágenes (512 en este caso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "47kUv48BvNLH"
      },
      "outputs": [],
      "source": [
        "R = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué la matriz de similaridad es de `1x101`?"
      ],
      "metadata": {
        "id": "mbpSvBGf3Zbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xar3GLe83YKn"
      },
      "outputs": [],
      "source": [
        "R = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTlDDclFyyi2"
      },
      "source": [
        "\n",
        "2. **Sugiera 2 templates para queries distintos al utilizado previamente y testee su rendimiento.**\n",
        "\n",
        "El formato de la query debe seguir el mismo que la utilizada previamente y debe cada una debe ser ingresada en un textbox distinto.\n",
        "\n",
        "Para testear sus templates, simplemente descomente el código más abajo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7CDTBVGDyyjC"
      },
      "outputs": [],
      "source": [
        "Q1 = \"\" #@param {type:\"string\"}\n",
        "Q2 = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ex1_queries = [Q1.format(label) for label in food_dataset.classes]\n",
        "# ex1_tokenized_queries = clip.tokenize(ex1_queries).to(device)\n",
        "\n",
        "# top1_acc, top5_acc = evaluate_model(\n",
        "#     model, test_dataset, ex1_tokenized_queries, batch_size=512)\n",
        "\n",
        "# top1_acc = top1_acc * 100\n",
        "# top5_acc = top5_acc * 100\n",
        "# print(f'Top-1 Accuracy: {top1_acc:.2f}%')\n",
        "# print(f'Top-5 Accuracy: {top5_acc:.2f}%')"
      ],
      "metadata": {
        "id": "mDivGgdqAYVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ex2_queries = [Q2.format(label) for label in food_dataset.classes]\n",
        "# ex2_tokenized_queries = clip.tokenize(ex2_queries).to(device)\n",
        "\n",
        "# top1_acc, top5_acc = evaluate_model(\n",
        "#     model, test_dataset, ex2_tokenized_queries, batch_size=512)\n",
        "\n",
        "# top1_acc = top1_acc * 100\n",
        "# top5_acc = top5_acc * 100\n",
        "# print(f'Top-1 Accuracy: {top1_acc:.2f}%')\n",
        "# print(f'Top-5 Accuracy: {top5_acc:.2f}%')"
      ],
      "metadata": {
        "id": "1AhaUY2PAnP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Stanford Cars\n",
        "\n",
        "Ahora probaremos como le va al modelo en otro dataset, con objetos totalmente distintos a los del anterior. En este caso se trata de un datasets de automoviles. De igual manera que el anterior, el modelo no fue entrenado con estas imágenes en ningún momento."
      ],
      "metadata": {
        "id": "qwbkd4RaAqYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "if not os.path.exists('data/stanford_cars.zip'):\n",
        "    !gdown --id 1JkcF--obwMvo2ZocIiKEli3EoE0e6ngt -O data/stanford_cars.zip\n",
        "\n",
        "!unzip -nq data/\\*.zip -d data"
      ],
      "metadata": {
        "id": "03g7t2S2euLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import StanfordCars\n",
        "\n",
        "cars_dataset = StanfordCars(\"data/\")"
      ],
      "metadata": {
        "id": "6TZRtdZ9AYg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual manera que para el dataset anterior, debemos definir nuestras *queries*."
      ],
      "metadata": {
        "id": "IUs5Spy6Ozfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_template = 'A photo of {}'\n",
        "queries = [query_template.format(label) for label in cars_dataset.classes]\n",
        "tokenized_queries = clip.tokenize(queries).to(device)"
      ],
      "metadata": {
        "id": "NLLcsd7YJE7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries[:10]:\n",
        "    print(query)"
      ],
      "metadata": {
        "id": "Zki2Kn_IJFAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización de Predicciones\n",
        "\n",
        "Igual que con el dataset anterior, seleccione una imágen del dataset utilizando el parámetro `index` para ver las predicciones del modelo. Si prefiere elegir una imágen al azar, active el campo de `random_sample`."
      ],
      "metadata": {
        "id": "ExqvXy95wt0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample = True #@param {type:\"boolean\"}\n",
        "index = 0 #@param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "hsGlWHSJwt1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if random_sample:\n",
        "    index = random.randint(0, len(cars_dataset))\n",
        "original_image, true_label = cars_dataset[index]\n",
        "image = preprocess(original_image)\n",
        "image_input = image.unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "qsUsTOkBwt1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = model.encode_text(tokenized_queries).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ],
      "metadata": {
        "id": "xXUAgjUDwt1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos el resultado del modelo."
      ],
      "metadata": {
        "id": "1zjEMEFdO9aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_label_name = cars_dataset.classes[true_label]\n",
        "\n",
        "print(bold('Predicciones Top-5\\n'))\n",
        "print(bold(f'Clase               '), bold(f'Probabilidad'))\n",
        "for prob, label in zip(top_probs[0].tolist(), top_labels[0].tolist()):\n",
        "    label_name = cars_dataset.classes[label]\n",
        "    if true_label_name == label_name:\n",
        "        print(bold(f'{label_name:20.20s} {prob:.4f}'))\n",
        "    else:\n",
        "        print(f'{label_name:20.20s} {prob:.4f}')\n",
        "\n",
        "print(bold(f'\\nClase verdadera: {true_label_name}'), end='\\n\\n')\n",
        "original_image"
      ],
      "metadata": {
        "id": "QPwhdwoCJFJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rendimiento en Test\n",
        "\n",
        "Ahora, evaluemos el modelo en el set de test del dataset Stanford Cars."
      ],
      "metadata": {
        "id": "dh4gAJKCPIVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "test_cars_dataset = StanfordCars('data/', split='test',\n",
        "                                 transform=preprocess)\n",
        "test_loader = DataLoader(test_cars_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "3ta6HzLPwycD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "in_top1 = in_top5 = total = 0.\n",
        "total_batches = len(test_cars_dataset) // batch_size\n",
        "for image_inputs, true_labels in tqdm(test_loader, total=total_batches):\n",
        "    image_inputs = image_inputs.to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_inputs).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = model.encode_text(tokenized_queries).float()\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "    top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)\n",
        "\n",
        "    label_match = (top_labels == true_labels.unsqueeze(-1))\n",
        "    in_top1 += float(label_match[:,0].sum())\n",
        "    in_top5 += float(label_match.any(-1).sum())\n",
        "    total += true_labels.numel()"
      ],
      "metadata": {
        "id": "_Twx6jO9wycE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top1_acc = in_top1 / total * 100\n",
        "top5_acc = in_top5 / total * 100\n",
        "print(f'Top-1 Accuracy: {top1_acc:.2f}%')\n",
        "print(f'Top-5 Accuracy: {top5_acc:.2f}%')"
      ],
      "metadata": {
        "id": "vZ1eRmRSxATJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actividad 4\n",
        "\n",
        "Prueba con tus propias imágenes. Utiliza el ceodigo a continuación para subir 5 imágenes distintas y generar 5 queries para estas, las queries deben ser distintas y debe haber una asociada a cada imagen. No necesariamente deben todas seguir el mismo *template*.\n",
        "\n",
        "Escribe abajo un pequeño análisis del resultados obtenido."
      ],
      "metadata": {
        "id": "CO9kxJAf6SDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = \"\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "msaeTwj-TJQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filenames = list(uploaded.keys())"
      ],
      "metadata": {
        "id": "VSfe6dqW4gxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "images = [Image.open(fn) for fn in filenames]\n",
        "image_inputs = torch.stack([preprocess(image) for image in images]).to(device)"
      ],
      "metadata": {
        "id": "NzsGzlWj4g4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries = [\n",
        "#     \"An image of an apple\",\n",
        "#     \"An image of an orange\",\n",
        "#     \"etc...\",\n",
        "# ]\n",
        "queries = [\n",
        "    \"A photo of apples, a type of fruit\",\n",
        "    \"A photo of pears, a type of fruit\",\n",
        "    \"A photo of papayas, a type of fruit\",\n",
        "    \"A photo of a pineapple, a type of fruit\",\n",
        "    \"A photo of a watermelon, a type of fruit\",\n",
        "]\n",
        "tokenized_queries = clip.tokenize(queries).to(device)"
      ],
      "metadata": {
        "id": "dyh9rxV45ct7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_inputs).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = model.encode_text(tokenized_queries).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "similarity = (image_features @ text_features.T).softmax(dim=-1).cpu().numpy().T"
      ],
      "metadata": {
        "id": "yw6Z4tXq43BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity.shape"
      ],
      "metadata": {
        "id": "7usTj_hH68Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = len(queries)\n",
        "\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
        "\n",
        "plt.yticks(range(count), queries, fontsize=14)\n",
        "plt.xticks([])\n",
        "for i, image in enumerate(images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, count - 0.5])\n",
        "plt.ylim([count + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between text and image features\", size=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cReTcjqi43zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_gS8E3UTaEQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}